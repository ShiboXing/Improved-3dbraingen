{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from utils import *\n",
    "from ipdb import set_trace\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "# from torch.autograd import Variable\n",
    "import nibabel as nib\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import dataloader\n",
    "from nilearn import plotting\n",
    "from ADNI_dataset import *\n",
    "from BRATS_dataset import *\n",
    "# from ATLAS_dataset import *\n",
    "from Model_alphaWGAN import *\n",
    "# from Model_VAEGAN import Encoder\n",
    "from utils import sinkhorn_pointcloud as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=4\n",
    "TEST_BATCH_SIZE=8\n",
    "gpu = True\n",
    "workers = 4\n",
    "\n",
    "LAMBDA= 10\n",
    "_eps = 1e-15\n",
    "Use_ADNI = True\n",
    "Use_BRATS = False\n",
    "Use_ATLAS = False\n",
    "\n",
    "#setting latent variable sizes\n",
    "latent_dim = 1000\n",
    "\n",
    "gpu_0 = 1\n",
    "gpu_1 = 1\n",
    "torch_seed = 0\n",
    "r_g = torch.manual_seed(torch_seed)\n",
    "checkpoint_pth = 'adni_wae_checkpoint/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'CN_list.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-36de2c7f6cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUse_ADNI\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrainset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mADNIdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# trainset, testset = data.random_split(dataset, [5, 5], generator=r_g)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n\u001b[1;32m      5\u001b[0m                                               shuffle=True,num_workers=workers)\n",
      "\u001b[0;32m/projects/shibo/3dbraingen/ADNI_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, augmentation, img_size, normalization)\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'FreeSurfer_Cross-Sectional_Processing_brainmask'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maugmentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CN_list.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m                 \u001b[0mrdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'CN_list.csv'"
     ]
    }
   ],
   "source": [
    "if Use_ADNI:\n",
    "    trainset = ADNIdataset(augmentation=True, img_size=64)\n",
    "    # trainset, testset = data.random_split(dataset, [5, 5], generator=r_g)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True,num_workers=workers)\n",
    "    # test_loader = torch.utils.data.DataLoader(testset,batch_size=TEST_BATCH_SIZE,\n",
    "    #                                           shuffle=True,num_workers=workers)\n",
    "if Use_BRATS:\n",
    "    #'flair' or 't2' or 't1ce'\n",
    "    trainset = BRATSdataset(imgtype='flair')\n",
    "    train_loader = torch.utils.data.DataLoader(trainset,batch_size = BATCH_SIZE, shuffle=True,\n",
    "                                               num_workers=workers)\n",
    "if Use_ATLAS:\n",
    "    trainset = ATLASdataset(augmentation=True)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(noise = latent_dim)\n",
    "D = Discriminator(is_dis=True)\n",
    "# E = Encoder(gpu_ind=gpu_0)\n",
    "E = Discriminator(out_class = latent_dim,is_dis=False, img_size=128)\n",
    "\n",
    "G.cuda(gpu_0)\n",
    "D.cuda(gpu_0)\n",
    "E.cuda(gpu_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_optimizer = optim.Adam(G.parameters(), lr=0.0002)\n",
    "d_optimizer = optim.Adam(D.parameters(), lr=0.0002)\n",
    "e_optimizer = optim.Adam(E.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "#remove Variable usage\n",
    "# real_y = torch.ones((BATCH_SIZE, 1)).cuda()#async=True))\n",
    "# fake_y = torch.zeros((BATCH_SIZE, 1)).cuda()#async=True))\n",
    "\n",
    "# criterion_bce = nn.BCELoss()\n",
    "criterion_l1 = nn.L1Loss()\n",
    "criterion_mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the highest savepoints of all models\n",
    "df = load_loss(path=checkpoint_pth)\n",
    "iteration = load_checkpoint(G, D, E, None, '_iter', path=checkpoint_pth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_iter = 1\n",
    "d_iter = 3\n",
    "TOTAL_ITER = 100000\n",
    "gen_load = inf_train_gen(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "while iteration <= TOTAL_ITER:\n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = True\n",
    "    for p in G.parameters():  \n",
    "        p.requires_grad = True\n",
    "        \n",
    "    ######i#########################################\n",
    "    # Train Generator \n",
    "    ###############################################\n",
    "    for iters in range(g_iter):\n",
    "        real_images = gen_load.__next__().cuda(gpu_0)\n",
    "        _batch_size = real_images.size(0)\n",
    "        if not _batch_size == BATCH_SIZE: \n",
    "            break\n",
    "        z_rand = torch.randn((_batch_size,latent_dim)).cuda(gpu_0)\n",
    "        z_hat = E(real_images).view(_batch_size,-1)\n",
    "        x_hat = G(z_hat)\n",
    "        x_rand = G(z_rand)\n",
    "        \n",
    "        d_recon_loss = D(x_hat).mean()\n",
    "        d_fake_loss = D(x_rand).mean()\n",
    "        l1_loss = 100 * criterion_l1(x_hat.cuda(gpu_1),real_images)\n",
    "        ### L2 loss(MSE loss) for reconstruction of the Encoder \n",
    "        z_ee, z_re = E(x_hat), E(x_rand)\n",
    "        z_e_l2, z_r_l2 = 50 * criterion_mse(z_hat, z_ee), 50 * criterion_mse(z_rand, z_re)\n",
    "        ###############################################\n",
    "        loss1 = l1_loss - d_fake_loss - d_recon_loss # + z_e_l2 + z_r_l2 \n",
    "        \n",
    "        G.zero_grad()\n",
    "        E.zero_grad()\n",
    "        if iters<g_iter-1:\n",
    "            loss1.backward()\n",
    "        else:\n",
    "            loss1.backward(retain_graph=True)\n",
    "        g_optimizer.step()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "    ######i#########################################\n",
    "    # Train Encoder\n",
    "    ###############################################\n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = True\n",
    "    for p in G.parameters(): \n",
    "        p.requires_grad = False\n",
    "    for iters in range(g_iter):\n",
    "        z_rand = torch.randn((_batch_size,latent_dim)).cuda(gpu_0)\n",
    "        z_hat = E(real_images).view(_batch_size,-1)\n",
    "        ### wasserstein loss between z_e and z_r ###################\n",
    "        w_dist = 100 * sp.sinkhorn_loss(torch.transpose(z_rand, 0, 1), torch.transpose(z_hat, 0, 1), 0.1, 1000, 100, gpu=gpu_0)\n",
    "        ###############################################\n",
    "        \n",
    "        e_loss = w_dist\n",
    "        E.zero_grad()\n",
    "        if iters<g_iter-1:\n",
    "            e_loss.backward()\n",
    "        else:\n",
    "            e_loss.backward(retain_graph=True)\n",
    "        e_optimizer.step()\n",
    "        \n",
    "        \n",
    "    ###############################################\n",
    "    # Train D\n",
    "    ###############################################\n",
    "    for p in D.parameters():  \n",
    "        p.requires_grad = True\n",
    "    for p in E.parameters():  \n",
    "        p.requires_grad = False\n",
    "    for p in G.parameters():  \n",
    "        p.requires_grad = False\n",
    "        \n",
    "    for iters in range(d_iter):\n",
    "        d_optimizer.zero_grad()\n",
    "        real_images = gen_load.__next__().cuda(gpu_0)\n",
    "        _batch_size = real_images.size(0)\n",
    "        \n",
    "        if not _batch_size == BATCH_SIZE: \n",
    "            break\n",
    "            \n",
    "        x_loss2 = -2*D(real_images).mean()+D(x_hat).mean()+D(x_rand).mean()\n",
    "        gradient_penalty_r = calc_gradient_penalty(D,real_images, x_rand, cuda_ind=gpu_1)\n",
    "        gradient_penalty_h = calc_gradient_penalty(D,real_images, x_hat, cuda_ind=gpu_1)\n",
    "        loss2 = x_loss2+gradient_penalty_r+gradient_penalty_h\n",
    "#         if iters < d_iter - 1:\n",
    "#             loss2.backward(retain_graph=True)\n",
    "#         else:\n",
    "        loss2.backward(retain_graph=True)\n",
    "        d_optimizer.step()\n",
    "        \n",
    "    ###############################################\n",
    "    # Visualization\n",
    "    ###############################################\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        lossStr = '[{}/{}]'.format(iteration,TOTAL_ITER) + '\\n\\\n",
    "        D: {:<8.3}'.format(loss2.item()) + '\\n\\\n",
    "        En_Ge: {:<8.3}'.format(loss1.item())\n",
    "        \n",
    "        print('lossStr', lossStr)\n",
    "        feat = np.squeeze((0.5*real_images[0]+0.5).cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        \n",
    "        plotting.plot_img(feat,title=\"X_Real\")\n",
    "        plotting.show()\n",
    "\n",
    "        feat = np.squeeze((0.5*x_hat[0]+0.5).detach().cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        plotting.plot_img(feat,title=\"DEC\")\n",
    "        \n",
    "        feat = np.squeeze((0.5*x_rand[0]+0.5).data.cpu().numpy())\n",
    "        feat = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        plotting.plot_img(feat,title=\"x_rand\")\n",
    "        plotting.show()\n",
    "        \n",
    "    ###############################################\n",
    "    # Save the losses\n",
    "    ###############################################\n",
    "    loss_dict = {\n",
    "        'index': [iteration],\n",
    "        'l1_loss': [l1_loss.item()],\n",
    "        'd_recon': [d_recon_loss.item()],\n",
    "        'd_fake': [d_fake_loss.item()],\n",
    "        'd_real': [-2*D(real_images).mean().item()],\n",
    "        'gp_r': [gradient_penalty_r.item() if gradient_penalty_r else 0],\n",
    "        'gp_h': [gradient_penalty_h.item() if gradient_penalty_h else 0],\n",
    "        'loss1': [loss1.item()],\n",
    "        'loss2': [loss2.item()],\n",
    "        'z_e_l2': [z_e_l2.item()],\n",
    "        'z_r_l2': [z_r_l2.item()],\n",
    "        'w_dist': [w_dist.item()],\n",
    "    }\n",
    "    df = add_loss(df, loss_dict)\n",
    "        \n",
    "\n",
    "    ###############################################\n",
    "    # Model Save\n",
    "    ###############################################\n",
    "    if iteration % 1000 == 0 and iteration:\n",
    "        viz_pca_tsne([E], trainset, is_tsne=True, latent_size=latent_dim, index=iteration, is_cd=True, gpu_ind=gpu_1)\n",
    "        viz_pca_tsne([G], trainset, is_tsne=True, latent_size=latent_dim, index=iteration, gpu_ind=gpu_1)\n",
    "        \n",
    "    if (iteration % 5000 == 0 or iteration == 1000) and iteration:\n",
    "        torch.save(G.state_dict(),f'./{checkpoint_pth}/G_iter'+str(iteration)+'.pth')\n",
    "        torch.save(D.state_dict(),f'./{checkpoint_pth}/D_iter'+str(iteration)+'.pth')\n",
    "        torch.save(E.state_dict(),f'./{checkpoint_pth}/E_iter'+str(iteration)+'.pth')\n",
    "        write_loss(df, path=checkpoint_pth)\n",
    "        calc_mmd(train_loader, G, iteration, count=1, gpu_ind=gpu_0, mode='rbf', path=checkpoint_pth, no_write=False)\n",
    "    iteration += 1\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_df = df\n",
    "print(loss_df.columns)\n",
    "loss_df[['loss1', 'loss2', 'loss3']].plot() \n",
    "loss_df[['l1_loss']].plot()\n",
    "loss_df[['z_e_l2','z_r_l2','w_dist']].plot()\n",
    "loss_df[['gp_r','gp_h']].plot()\n",
    "loss_df[['loss2']].plot()\n",
    "plt.figure(figsize=(400, 400))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
