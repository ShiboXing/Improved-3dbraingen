{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import dataloader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nilearn import plotting\n",
    "from ADNI_dataset import *\n",
    "from BRATS_dataset import *\n",
    "from ATLAS_dataset import *\n",
    "from Model_alphaWGAN import *\n",
    "import pytorch_ssim\n",
    "import matplotlib.pyplot as plt\n",
    "from ipdb import set_trace\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# used to store the MMD ans MS-SSIM\n",
    "try:\n",
    "    os.mkdir('./test_data')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "workers = 4\n",
    "BATCH_SIZE = 8\n",
    "latent_dim = 1000\n",
    "\n",
    "Use_BRATS = False\n",
    "Use_ATLAS = False\n",
    "gpu = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainset = ADNIdataset(augmentation=True)\n",
    "train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,num_workers=workers)\n",
    "#'flair' or 't2' or 't1ce'\n",
    "# trainset = BRATSdataset(imgtype='flair')\n",
    "# train_loader = torch.utils.data.DataLoader(trainset,batch_size = BATCH_SIZE, shuffle=True,\n",
    "#                                                num_workers=workers)\n",
    "\n",
    "# trainset = ATLASdataset(augmentation=True)\n",
    "# train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "#                                           shuffle=True,num_workers=workers)\n",
    "\n",
    "gen_load = inf_train_gen(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained Generator Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------\n",
    "#Choose the Model you want!\n",
    "from Model_alphaWGAN import *\n",
    "# from Model_alphaWGAN import Discriminator\n",
    "# from Model_alphaGAN import Generator\n",
    "# from Model_VAEGAN import Generator\n",
    "# from Model_VAEGAN import Encoder\n",
    "# from Model_WGAN import Generator\n",
    "#-------------------------------------------\n",
    "\n",
    "G = Generator(noise=latent_dim).cuda(gpu)\n",
    "# E = Encoder(out_class = latent_dim).cuda(gpu)\n",
    "E = Discriminator(out_class = latent_dim, is_dis=False).cuda(gpu)\n",
    "#-----------------------\n",
    "#Load Pre-trained model\n",
    "#-----------------------\n",
    "\n",
    "#------------Trained Model of ADNI dataset---------------------\n",
    "# G.load_state_dict(torch.load('./vae_checkpoint_gp/G_iter.pth')) \n",
    "G.load_state_dict(torch.load('./alpha_checkpoint/G_iter100000.pth')) \n",
    "E.load_state_dict(torch.load('./alpha_checkpoint/E_iter100000.pth')) \n",
    "\n",
    "#------------Trained Model of ATLAS dataset---------------------\n",
    "# G.load_state_dict(torch.load('./checkpoint/Ours_at_G.pth'))\n",
    "\n",
    "#------------Trained Models of BRATS dataset---------------------\n",
    "# G.load_state_dict(torch.load('./checkpoint/Ours_fl_G.pth'))\n",
    "# G.load_state_dict(torch.load('./checkpoint/Ours_t2_G.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake Image - slice series visualization\n",
    "\n",
    "You can change the axis (x , y , z ) by changing  display_mode = 'x' / 'y' / 'z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Show_color = False\n",
    "\n",
    "noise = torch.randn(1, latent_dim).cuda(gpu)\n",
    "fake_image = G(noise)\n",
    "featmask = np.squeeze(fake_image[0].data.cpu().numpy())\n",
    "featmask = nib.Nifti1Image(featmask,affine = np.eye(4))\n",
    "\n",
    "arr1 = [4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]\n",
    "arr2 = [34,36,38,40,42,44,46,48,50,52,54,56,58,60]\n",
    "if Show_color:\n",
    "    disp = plotting.plot_img(featmask,cut_coords=arr1,draw_cross=False,annotate=False,black_bg=True,display_mode='x')\n",
    "    # disp.annotate(size=25,left_right=False,positions=True)\n",
    "    plotting.show()\n",
    "    disp=plotting.plot_img(featmask,cut_coords=arr2,draw_cross=False,annotate=False,black_bg=True,display_mode='x')\n",
    "    # disp.annotate(size=25,left_right=False)\n",
    "    plotting.show()\n",
    "else:\n",
    "    disp = plotting.plot_anat(featmask,cut_coords=arr1,draw_cross=False,annotate=False,black_bg=True,display_mode='x')\n",
    "    plotting.show()\n",
    "    # disp.annotate(size=25,left_right=False)\n",
    "    disp=plotting.plot_anat(featmask,cut_coords=arr2,draw_cross=False,annotate=False,black_bg=True,display_mode='x')\n",
    "    # disp.annotate(size=25,left_right=False)\n",
    "    plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fake Image - Center cut slices Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "noise = Variable(torch.randn((1, 1000)).cuda(gpu))\n",
    "# fake_image = G(noise)\n",
    "featmask = np.squeeze(fake_image[0].data.cpu().numpy())\n",
    "featmask = nib.Nifti1Image(featmask,affine = np.eye(4))\n",
    "plotting.plot_img(featmask,cut_coords=(32,32,32),draw_cross=False,annotate=False,black_bg=True)\n",
    "plotting.plot_anat(featmask,cut_coords=(32,32,32),draw_cross=False,annotate=False,black_bg=True)\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real Image - Slice series visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=True, num_workers=workers)\n",
    "\n",
    "Show_color = False\n",
    "\n",
    "image = gen_load.__next__()\n",
    "featmask = np.squeeze(image[0].data.cpu().numpy())\n",
    "featmask = nib.Nifti1Image(featmask,affine = np.eye(4))\n",
    "arr1 = [4,6,8,10,12,14,16,18,20,22,24,26,28,30,32]\n",
    "arr2 = [34,36,38,40,42,44,46,48,50,52,54,56,58,60]\n",
    "\n",
    "if Show_color:\n",
    "    disp = plotting.plot_img(featmask,cut_coords=arr1,draw_cross=False,annotate=False,black_bg=True,display_mode='x')\n",
    "    # disp.annotate(size=25,left_right=False,positions=True)\n",
    "    plotting.show()\n",
    "    disp=plotting.plot_img(featmask,cut_coords=arr2,draw_cross=False,annotate=False,black_bg=True,display_mode='x')\n",
    "    # disp.annotate(size=25,left_right=False)\n",
    "    plotting.show()\n",
    "else:\n",
    "    disp = plotting.plot_anat(featmask,cut_coords=arr1,draw_cross=False,annotate=False,black_bg=True,display_mode='x')\n",
    "    plotting.show()\n",
    "    # disp.annotate(size=25,left_right=False)\n",
    "    disp=plotting.plot_anat(featmask,cut_coords=arr2,draw_cross=False,annotate=False,black_bg=True,display_mode='x')\n",
    "    # disp.annotate(size=25,left_right=False)\n",
    "    plotting.show()\n",
    "\n",
    "plotting.plot_img(featmask,cut_coords=(32,32,32),draw_cross=False,annotate=False,black_bg=True)\n",
    "plotting.plot_anat(featmask,cut_coords=(32,32,32),draw_cross=False,annotate=False,black_bg=True)\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS-SSIM Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "G.load_state_dict(torch.load('./mmd_checkpoint/G_iter42000.pth'))\n",
    "sum_ssim = 0\n",
    "for i in range(1000):\n",
    "    noise = Variable(torch.randn((2, 1000)).cuda(gpu))\n",
    "    images = gen_load.__next__().cuda(gpu)\n",
    "#   images = G(noise)\n",
    "    img1 = images[0]\n",
    "    img2 = images[1]\n",
    "\n",
    "    msssim = pytorch_ssim.msssim_3d(img1,img2)\n",
    "    sum_ssim = sum_ssim+msssim\n",
    "    if i % 100 == 0:\n",
    "        print(sum_ssim/1000)\n",
    "print(f'final ssim: {sum_ssim/1000}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS-SSIM Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ssim_pth = 'alpha_checkpoint_2'\n",
    "last_ind = read_ssim(path=ssim_pth)\n",
    "last_ind = 1000 if not last_ind else last_ind\n",
    "inds = [1000] + list(range(5000, 200001, 5000))\n",
    "for i in inds[inds.index(last_ind):]:\n",
    "#     G.load_state_dict(torch.load(f'./{ssim_pth}/G_VG_ep_{i}.pth'))\n",
    "    G.load_state_dict(torch.load(f'./{ssim_pth}/G_iter{i}.pth'))\n",
    "    calc_ssim(G, i, ssim_pth, no_write=False, gpu=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MS-SSIM for real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=True, num_workers=workers)\n",
    "test_loader = inf_train_gen(train_loader)\n",
    "\n",
    "i = 0\n",
    "sum_ssim = 0\n",
    "while i < 494:\n",
    "    img1, img2 = test_loader.__next__()\n",
    "    msssim = pytorch_ssim.msssim_3d(img1,img2)\n",
    "    sum_ssim = sum_ssim+msssim\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i, sum_ssim/ (494 * 2))\n",
    "        \n",
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=True, num_workers=workers)\n",
    "test_loader = inf_train_gen(train_loader)\n",
    "while i < 494 * 2:\n",
    "    img1, img2 = test_loader.__next__()\n",
    "    msssim = pytorch_ssim.msssim_3d(img1,img2)\n",
    "    sum_ssim = sum_ssim+msssim\n",
    "    i += 1\n",
    "    if i % 100 == 0:\n",
    "        print(i, sum_ssim/ (494 * 2))\n",
    "print(sum_ssim/(494 * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum-Mean Discrepancy Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate averaged mmd score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode = 'rbf'\n",
    "mmd_pth = 'wl_checkpoint'\n",
    "G.load_state_dict(torch.load(f'./{mmd_pth}/G_iter100000.pth')) \n",
    "for i in [1, 10, 100, 1000]:\n",
    "    calc_old_mmd(train_loader, G, i, count=10, mode=mode, gpu_ind=gpu, path=mmd_pth, no_write=True, z_r=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create mmd cruve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode ='rbf'\n",
    "mmd_pth = 'wl_checkpoint_mse_3_2'\n",
    "for i in range(2000, 100000, 1000):\n",
    "    i = int(i / 1000) if 'vae' in mmd_pth else i\n",
    "    G.load_state_dict(torch.load(f'./{mmd_pth}/G_iter{i}.pth'))\n",
    "#     G.load_state_dict(torch.load(f'./{mmd_pth}/G_VG_ep_{i}.pth'))\n",
    "    calc_mmd(train_loader, G, i, count=1, gpu_ind=gpu, mode=mode, path=mmd_pth, no_write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode ='rbf'\n",
    "mmd_pth = 'wl_checkpoint_mse_3'\n",
    "last_ind = int(read_mmd(mmd_pth, name=f'{mode}_mmd.csv'))\n",
    "end = 101000\n",
    "last_ind = last_ind * 1000 if 'vae' in mmd_pth else last_ind\n",
    "if not last_ind:\n",
    "    inds = [1000] + list(range(5000, end, 5000))\n",
    "elif last_ind == 1000:\n",
    "    inds = range(5000, end, 5000)\n",
    "else: \n",
    "    inds = range(last_ind + 5000, end, 5000)\n",
    "for i in inds:\n",
    "    i = int(i / 1000) if 'vae' in mmd_pth else i\n",
    "    G.load_state_dict(torch.load(f'./{mmd_pth}/G_iter{i}.pth'))\n",
    "#     G.load_state_dict(torch.load(f'./{mmd_pth}/G_VG_ep_{i}.pth'))\n",
    "    calc_mmd(train_loader, G, i, count=10, gpu_ind=gpu, mode=mode, path=mmd_pth, no_write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize mmd curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = load_csv('./trained/wl_gan_mmd.csv')\n",
    "plt.figure()\n",
    "df['mmd_score'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne_pth, model = 'wl_checkpoint_mse', E\n",
    "model_code = 'E' if model == E else 'G'\n",
    "model.load_state_dict(torch.load(f'./{tsne_pth}/{model_code}_iter100000.pth'))\n",
    "for i in [0.1]: #, 1, 10, 100, 1000, 5000, 10000]:\n",
    "    start = time()\n",
    "    viz_pca_tsne(model, trainset, index=i, is_cd=(model == E), is_tsne=True, latent_size=latent_dim, gpu_ind=gpu, perplexity=30, z_r=1)\n",
    "    print(f'time cost: {time() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### viz all T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/usr/local/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0, sample_mean (blue): -0.8607863187789917 sample_var: 0.058691687881946564, real_mean (yellow): -0.8591290712356567 real_var: 0.05611636862158775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/usr/local/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0, sample_mean (blue): -0.8698945045471191 sample_var: 0.051263563334941864, real_mean (yellow): -0.8593956232070923 real_var: 0.05580316111445427\n"
     ]
    }
   ],
   "source": [
    "checkpoints = ['alpha_checkpoint', 'wl_checkpoint', 'vae_checkpoint', 'wl_checkpoint_mse']\n",
    "is_E = False\n",
    "models = fetch_models(checkpoints, is_E=is_E)\n",
    "viz_pca_tsne(models, trainset, index=0, is_cd=is_E, is_pca=True, is_tsne=True, batch_size=1, latent_size=latent_dim, gpu_ind=gpu, perplexity=30, z_r=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/projects/shibo/3dbraingen/utils/__init__.py\u001b[0m(141)\u001b[0;36msample_from_model\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    139 \u001b[0;31m            \u001b[0;32mif\u001b[0m \u001b[0mis_real\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    140 \u001b[0;31m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 141 \u001b[0;31m                \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    142 \u001b[0;31m                \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtuple\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# guard for vae_gan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    143 \u001b[0;31m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# sample for X space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> model\n",
      "'alpha_checkpoint'\n",
      "ipdb> exit\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_pth = 'wl_checkpoint'\n",
    "for i in range(100000, 0, -5000):\n",
    "# for i in range(122, -1, -10):\n",
    "    G.load_state_dict(torch.load(f'./{pca_pth}/G_iter{i}.pth'))\n",
    "    viz_pca_tsne(G, trainset, latent_size=latent_dim, gpu_ind=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_pca_cluster(x, PC_list, interval=3):\n",
    "    for i in range(x, x+interval):\n",
    "        sample_ind = PC_list[i][2]\n",
    "        feat = sample_df.iloc[int(sample_ind)].to_numpy().reshape((64, 64, 64))\n",
    "        featmask = nib.Nifti1Image(feat,affine = np.eye(4))\n",
    "        plotting.plot_img(featmask, title=f'x_val: {round(PC_list[i][0], 2)} y_val: {round(PC_list[i][1], 2)} sample_index: {int(sample_ind)}')\n",
    "        plotting.show()\n",
    "\n",
    "# concatenate the sample index \n",
    "PC_list = list(np.concatenate((PCs, np.array([i for i in range(512)]).reshape(512, 1)), 1ghgng\n",
    "# sort by x or y of PCAs\n",
    "PC_list.sort(key=lambda x: x[1])\n",
    "\n",
    "# for i in range(0, 512, 60):\n",
    "#     show_pca_cluster(i, PC_list, 1)\n",
    "\n",
    "show_pca_cluster(0, PC_list, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize all training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "viz_all_imgs('../../ADNI', [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_df = load_loss(path='wl_checkpoint_2')\n",
    "# print(loss_df)\n",
    "# alpha_wgan\n",
    "plt.figure(figsize=(400, 400))\n",
    "loss_df[['loss2','gp_h','gp_r',]].plot() \n",
    "loss_df[['loss1','l1_loss','d_recon','d_fake','d_real','d_loss',]].plot()\n",
    "loss_df[['d_recon','d_fake','l1_loss']].plot()\n",
    "# vae_gan\n",
    "# loss_df[['d_real_loss', 'd_fake', 'd_recon', 'err_enc']].plot() \n",
    "\n",
    "# plt.figure()\n",
    "# # loss_df[['loss1', 'mmd_loss']].plot()\n",
    "# plt.figure()\n",
    "# loss_df[['gp_r', 'gp_h', 'l1_loss']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Z space pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "E.load_state_dict(torch.load(f'./wl_checkpoint_mse/E_iter100000.pth'))\n",
    "for i in [1, 10, 100, 1000]:\n",
    "    viz_pca(E, trainset, latent_size=latent_dim, index=100000, is_cd=True, gpu_ind=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for VAEGAN\n",
    "for i in range(100, -1, -10):\n",
    "    E.load_state_dict(torch.load(f'./vae_checkpoint_1/E_VG_ep_{i}.pth'))\n",
    "    viz_pca(E, trainset, latent_size=latent_dim, index=i, is_cd=True, gpu_ind=gpu, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for AlphaWGAN\n",
    "for i in range(42000, 0, -2000):\n",
    "    E.load_state_dict(torch.load(f'./wl_checkpoint/E_iter{i}.pth'))\n",
    "    viz_pca(E, trainset, latent_size=latent_dim, index=i, is_cd=True, gpu_ind=gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### delete CD and D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the paddings of fake images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G = Generator(noise=1000).cuda(gpu)\n",
    "G.load_state_dict(torch.load('./trained/G_iter12500.pth'))\n",
    "\n",
    "def get_padding(feat, axis=0):\n",
    "    feat = np.swapaxes(feat, 0, axis)\n",
    "    output = [0, 0]\n",
    "    for i in range(feat.shape[0]):\n",
    "#         print(feat[i].sum())\n",
    "        if feat[i].sum() > -4000:\n",
    "            output[0] = i\n",
    "            break\n",
    "    for i in range(feat.shape[0] - 1, -1, -1):\n",
    "        if feat[i].sum() > -4000:\n",
    "            output[1] = feat.shape[0] - i - 1\n",
    "            break\n",
    "    return output\n",
    "    \n",
    "for i in range(512):\n",
    "    noise = torch.rand((1, 1000)).cuda(gpu)\n",
    "    img = G(noise)\n",
    "    featmask = np.squeeze((0.5*img+0.5).detach().cpu().numpy())\n",
    "    print(f'ind: {i} padding 0-axis: {get_padding(featmask, 0)}', end=' ')\n",
    "    print(f'ind: {i} padding 1-axis: {get_padding(featmask, 1)}', end=' ')\n",
    "    print(f'ind: {i} padding 2-axis: {get_padding(featmask, 2)}')\n",
    "#     featmask = nib.Nifti1Image(featmask,affine = np.eye(4))\n",
    "#     plotting.plot_img(featmask, title=f'ind: {i}')\n",
    "#     plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### intensity range check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainset = ADNIdataset(augmentation=True, img_size=64, normalization=False)\n",
    "train_loader = torch.utils.data.DataLoader(trainset,batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,num_workers=workers)\n",
    "for featmask in inf_train_gen(train_loader):\n",
    "#     noise = torch.rand((1, 1000)).cuda()\n",
    "#     featmask= G(noise)\n",
    "#     featmask = np.squeeze((0.5*featmask+0.5).detach().cpu().numpy())\n",
    "    lo, hi = featmask.min(), featmask.max()\n",
    "    print(f'lo: {lo} hi: {hi}')\n",
    "#     featmask = nib.Nifti1Image(featmask,affine = np.eye(4))\n",
    "#     plotting.plot_img(featmask, title=f'ind: {i}')\n",
    "#     plotting.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import sinkhorn_pointcloud as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "from ipdb import set_trace\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "gpu=1\n",
    "a = Variable(torch.randn((1000, 4)).cuda(gpu), requires_grad=True)\n",
    "b = Variable(torch.randn((1000, 4)).cuda(gpu) * 0.01, requires_grad=True) \n",
    "g_optimizer = optim.Adam([a, b], lr=0.0002)\n",
    "print(a.shape, b.shape)\n",
    "for i in range(1000):\n",
    "    g_optimizer.zero_grad()\n",
    "    w_dist = sp.sinkhorn_loss(a, b, 0.1, 1000, 100, gpu=gpu)\n",
    "    w_dist.backward()\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print(w_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = torch.randn((1, 5), requires_grad=True)\n",
    "c = a * 2\n",
    "c.requires_grad = True;\n",
    "b = c.sum()\n",
    "b.backward()\n",
    "print(a.grad)\n",
    "print(c.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
